<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="TW">
<meta name="dcterms.date" content="2025-03-29">

<title>Q3. Data Augmentation (5 points) – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Q3. Data Augmentation (5 points)</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">exam</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>TW </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 29, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="q3.-data-augmentation-5-points" class="level1">
<h1>Q3. Data Augmentation (5 points)</h1>
<p>We all know that adequate training data is a precondition for training machine learning models. But in real-world problems, the data that can be used to train the model is often not enough. Suppose you are doing a classification task and your training dataset is extremely insufficient. Please explain how you will expand the amount of data.</p>
<p><strong>Notes :</strong></p>
<p>You do NOT need to code in this question, but you need to answer in detail. Please give at least two specific examples to illustrate, such as image classification, text classification and so on. You can also refer to other materials to answer this question, if you do so, please also list your references.</p>
<p><strong>Submissions :</strong></p>
<ol type="1">
<li>Put your answer and references in Q3_readme.pdf , and put it in folder Q3 .</li>
<li>No page limit for the answer.</li>
</ol>
<hr>
<p>Let’s address Q3, which asks about data augmentation in the context of a classification task where the training dataset is extremely insufficient. The goal is to explain how to expand the amount of data, provide at least two specific examples (e.g., for image classification, text classification, etc.), and document the answer in a <code>Q3_readme.pdf</code> file to be placed in a folder named <code>Q3</code>. No coding is required, but the explanation must be detailed. Let’s break this down and provide a thorough response.</p>
<hr>
<section id="answer-to-q3-data-augmentation-for-insufficient-training-data" class="level3">
<h3 class="anchored" data-anchor-id="answer-to-q3-data-augmentation-for-insufficient-training-data">Answer to Q3: Data Augmentation for Insufficient Training Data</h3>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">Introduction</h4>
<p>Data augmentation is a critical technique in machine learning, especially when the training dataset is insufficient. In real-world problems, collecting large amounts of labeled data can be expensive, time-consuming, or impractical. For classification tasks, an insufficient dataset can lead to overfitting, where the model learns to memorize the training data rather than generalize to unseen data. Data augmentation addresses this by artificially expanding the training dataset through transformations or synthetic data generation, thereby improving the model’s robustness and generalization. In this response, I will explain how data augmentation can be used to expand the amount of data for a classification task and provide two specific examples: one for image classification and one for text classification.</p>
</section>
<section id="what-is-data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="what-is-data-augmentation">What is Data Augmentation?</h4>
<p>Data augmentation involves creating new training samples by applying transformations to the existing data while preserving the labels. These transformations are designed to mimic real-world variations that the model might encounter, making the model more robust. For example, in image classification, a picture of a dog might be rotated or flipped to create new samples that still represent a dog. In text classification, synonyms might be substituted to create new sentences with the same meaning. The key is to ensure that the augmented data remains representative of the original class.</p>
</section>
<section id="why-use-data-augmentation-for-insufficient-data" class="level4">
<h4 class="anchored" data-anchor-id="why-use-data-augmentation-for-insufficient-data">Why Use Data Augmentation for Insufficient Data?</h4>
<p>When the training dataset is extremely insufficient, the model may fail to learn the underlying patterns of the data, leading to poor performance on unseen data. Data augmentation helps by:</p>
<ol type="1">
<li><strong>Increasing Dataset Size</strong>: More data allows the model to learn better representations and reduces overfitting.</li>
<li><strong>Introducing Variability</strong>: Augmentation mimics real-world variations (e.g., lighting changes in images, paraphrasing in text), making the model more robust.</li>
<li><strong>Balancing Classes</strong>: In imbalanced datasets, augmentation can generate more samples for underrepresented classes.</li>
<li><strong>Reducing Data Collection Costs</strong>: Instead of collecting new labeled data, augmentation creates synthetic data at a lower cost.</li>
</ol>
</section>
<section id="how-to-expand-the-amount-of-data-using-data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="how-to-expand-the-amount-of-data-using-data-augmentation">How to Expand the Amount of Data Using Data Augmentation</h4>
<p>To expand the amount of data, we can apply domain-specific transformations to the existing dataset. The choice of augmentation techniques depends on the type of data (e.g., images, text, audio) and the classification task. Below, I outline the general approach and then provide two specific examples.</p>
<p><strong>General Approach</strong>:</p>
<ol type="1">
<li><strong>Understand the Data and Task</strong>: Identify the type of data (e.g., images, text) and the classification task (e.g., binary or multi-class).</li>
<li><strong>Select Appropriate Transformations</strong>: Choose augmentation techniques that preserve the label while introducing meaningful variations. For example, rotating an image of a cat still results in an image of a cat, but changing the word “cat” to “dog” in a sentence changes the meaning and label.</li>
<li><strong>Apply Transformations</strong>: Use libraries or tools to apply the transformations (e.g., image processing libraries for images, NLP libraries for text).</li>
<li><strong>Balance the Dataset</strong>: If the dataset is imbalanced, apply augmentation more aggressively to underrepresented classes.</li>
<li><strong>Validate the Augmented Data</strong>: Ensure that the augmented data is still representative of the original classes and doesn’t introduce noise or bias.</li>
</ol>
</section>
<section id="example-1-image-classification-e.g.-classifying-cats-vs.-dogs" class="level4">
<h4 class="anchored" data-anchor-id="example-1-image-classification-e.g.-classifying-cats-vs.-dogs">Example 1: Image Classification (e.g., Classifying Cats vs.&nbsp;Dogs)</h4>
<p>Suppose we have a small dataset of 100 images (50 cats, 50 dogs) for a binary image classification task, which is insufficient to train a deep learning model like a convolutional neural network (CNN). We can use data augmentation to expand the dataset as follows:</p>
<ul>
<li><p><strong>Transformations</strong>:</p>
<ol type="1">
<li><strong>Rotation</strong>: Rotate each image by random angles (e.g., -30° to 30°). A cat image rotated by 15° still depicts a cat.</li>
<li><strong>Flipping</strong>: Apply horizontal flips to create mirror images. A horizontally flipped dog image still depicts a dog.</li>
<li><strong>Brightness Adjustment</strong>: Adjust the brightness of the image (e.g., increase or decrease by 20%). This mimics different lighting conditions.</li>
<li><strong>Cropping and Resizing</strong>: Randomly crop a portion of the image and resize it back to the original dimensions. This simulates different perspectives.</li>
<li><strong>Color Jittering</strong>: Slightly alter the hue, saturation, or contrast to simulate color variations.</li>
</ol></li>
<li><p><strong>Implementation</strong>:</p>
<ul>
<li>Using a library like <code>albumentations</code> or <code>torchvision</code> (in Python), we can apply these transformations. For example, a single image of a cat can be transformed into 5 new images: one rotated, one flipped, one with adjusted brightness, one cropped, and one with color jittering.</li>
<li>If we apply 5 transformations to each of the 100 images, we can expand the dataset to 500 images (100 original + 400 augmented).</li>
</ul></li>
<li><p><strong>Impact</strong>:</p>
<ul>
<li>The model now has more data to learn from, reducing overfitting.</li>
<li>The transformations introduce variability (e.g., different angles, lighting), making the model more robust to real-world variations.</li>
<li>For example, a CNN trained on this augmented dataset will better handle images of cats taken at different angles or under different lighting conditions.</li>
</ul></li>
</ul>
</section>
<section id="example-2-text-classification-e.g.-sentiment-analysis" class="level4">
<h4 class="anchored" data-anchor-id="example-2-text-classification-e.g.-sentiment-analysis">Example 2: Text Classification (e.g., Sentiment Analysis)</h4>
<p>Suppose we have a small dataset of 200 text samples (100 positive reviews, 100 negative reviews) for a binary sentiment classification task, which is insufficient to train a model like a recurrent neural network (RNN) or transformer. We can use data augmentation to expand the dataset as follows:</p>
<ul>
<li><p><strong>Transformations</strong>:</p>
<ol type="1">
<li><strong>Synonym Replacement</strong>: Replace words with their synonyms while preserving the sentiment. For example, in the sentence “I love this movie,” replace “love” with “adore” to get “I adore this movie” (still positive).</li>
<li><strong>Paraphrasing</strong>: Rewrite the sentence in a different way while keeping the meaning. For example, “This film is amazing” can be paraphrased as “This movie is fantastic” (still positive).</li>
<li><strong>Back-Translation</strong>: Translate the sentence to another language and back to the original language. For example, “I hate this product” (negative) might become “I dislike this item” after translating to French and back to English (still negative).</li>
<li><strong>Word Insertion/Deletion</strong>: Insert or delete neutral words that don’t change the sentiment. For example, “I really like this” can become “I really truly like this” (still positive).</li>
</ol></li>
<li><p><strong>Implementation</strong>:</p>
<ul>
<li>Using NLP libraries like <code>nlpaug</code> or <code>textattack</code> (in Python), we can apply these transformations. For example, a single positive review can be augmented into 3 new samples: one with synonym replacement, one with paraphrasing, and one with back-translation.</li>
<li>If we apply 3 transformations to each of the 200 samples, we can expand the dataset to 800 samples (200 original + 600 augmented).</li>
</ul></li>
<li><p><strong>Impact</strong>:</p>
<ul>
<li>The model now has more text samples to learn from, improving its ability to generalize.</li>
<li>The transformations introduce linguistic variability (e.g., different word choices, sentence structures), making the model more robust to diverse writing styles.</li>
<li>For example, a transformer model trained on this augmented dataset will better handle reviews with varied vocabulary or phrasing.</li>
</ul></li>
</ul>
</section>
<section id="additional-considerations" class="level4">
<h4 class="anchored" data-anchor-id="additional-considerations">Additional Considerations</h4>
<ul>
<li><strong>Preserving Labels</strong>: Ensure that the transformations do not change the class label. For example, flipping an image is fine, but negating a sentence (e.g., “I love this” to “I don’t love this”) changes the sentiment and label.</li>
<li><strong>Avoiding Over-Augmentation</strong>: Too much augmentation can introduce noise or bias. For example, excessive rotation of an image might make it unrecognizable, or over-paraphrasing might alter the meaning of a sentence.</li>
<li><strong>Domain-Specific Augmentation</strong>: The choice of transformations depends on the domain. For audio classification, you might add background noise or change the pitch; for tabular data, you might add Gaussian noise to numerical features.</li>
</ul>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<p>Data augmentation is a powerful technique to expand an insufficient training dataset for classification tasks. By applying transformations like rotation and flipping for image classification, or synonym replacement and paraphrasing for text classification, we can significantly increase the dataset size and introduce variability, leading to better model performance. These techniques are especially valuable in real-world scenarios where collecting more labeled data is not feasible.</p>
</section>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references">References</h4>
<ul>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press. (Chapter on data augmentation techniques.)</li>
<li>Shorten, C., &amp; Khoshgoftaar, T. M. (2019). “A survey on Image Data Augmentation for Deep Learning.” <em>Journal of Big Data</em>. (For image augmentation techniques.)</li>
<li>Wei, J., &amp; Zou, K. (2019). “EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.” <em>arXiv preprint arXiv:1901.11196</em>. (For text augmentation techniques.)</li>
</ul>
<hr>
</section>
</section>
<section id="submission" class="level3">
<h3 class="anchored" data-anchor-id="submission">Submission</h3>
<p>The answer above should be documented in a file named <code>Q3_readme.pdf</code> and placed in a folder named <code>Q3</code>. Since no coding is required, the folder will contain only the PDF file.</p>
<p><strong>Folder Structure</strong>:</p>
<pre><code>Q3/
└── Q3_readme.pdf</code></pre>
<p>To create the PDF: 1. Copy the answer above into a document editor (e.g., Microsoft Word, Google Docs). 2. Format it for clarity (e.g., use headings, bullet points). 3. Export the document as a PDF named <code>Q3_readme.pdf</code>. 4. Place the PDF in a folder named <code>Q3</code>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>