---
title: "EMNLP-IJCNLP 2019"
author: "TW"
date: "2025-04-04"
categories: [work]
---

### Blog Post: Day 1 at EMNLP-IJCNLP 2019 – A Packed Sunday of NLP Insights  
*Sunday, November 3, 2019 – First Day of the Conference*  

I had the privilege of attending the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) in Hong Kong. Day 1 was a whirlwind of sessions spanning annotation challenges, fact-checking, clinical NLP, and more. Below are my reflections based on the quick notes I jotted down during Sunday’s talks—perfect for jogging my memory later!

---

#### 9:00 – Cross-Document Annotation: Tackling Cancer References  
The day kicked off with a session on cross-document annotation, using cancer-related terms as a case study (e.g., "resection" = "surgery"). The problem? Aligning terms across documents is messy when meanings shift subtly. Two solutions stood out:  

1. Add a **CONTAIN-SUBEVENT** relation to model hierarchical events.  
2. Rely on dictionary definitions over gut intuition for consistency.  

A stat that stuck with me: "whole-part" relations scored 0%, while "identical" terms hit 93%. Why not 100% for identical? It’s a hint that even synonyms aren’t as simple as they seem—context reigns supreme.

---

#### 11:00 – Fact-Checking: Automation, Interpretability, and Bias  
Next was a dive into automated fact-checking—a hot topic given fake news debates. The goal: a fast, interpretable system that handles multimodal data (text, images, videos) and multiple languages. Bias was a big concern—systems can unintentionally favor or discriminate against groups.  

Two approaches caught my eye:  

1. **Hierarchical Stance Detection Model**:  
   - Asks: Does this evidence support the claim? Options: *agree*, *disagree*, *discuss*, or *unrelated*.  
   - Uses a relatedness layer feeding into a stance layer, with KL divergence as the loss function.  
   - Combines two weighted classification losses and MMD (Maximum Mean Discrepancy) regularization to address class imbalance.  

2. **Reply-Aided Misinformation Detection**:  
   - Integrates claims and replies to generate a probability distribution of veracity.  
   - A Bayesian deep model with multiple Gaussian distributions (mean and variance) powers it.  
   - Bi-LSTM encodes claim + reply, and an MLP predicts truthfulness, updating prior beliefs.  

Future work? Explainability, balancing bias vs. truth, and scaling to multimodal/multilingual data. Early detection remains elusive but critical.

---

#### 11:45 – Wikimedia: The Medical Info Hub  
This talk highlighted Wikimedia’s role as a top online medical resource. Tools like the `mediawiki-utilities` Python package and ORES (built with scikit-learn) help maintain it, but challenges persist: manual "citation needed" tags, circular reporting, and sock puppets (fake accounts). An RNN model was mentioned—maybe for quality scoring or spotting patterns? It’s a wild mix of NLP and crowdsourcing.

---

#### 12:00 – FEVER 2.0: Build, Break, Fix  
The FEVER 2.0 shared task was a three-part sprint:  

- **12:00 – Overview**: Build a fact-checking system, break it with adversarial attacks, then fix it.  
- **12:10 – Breaker**: Adversarial attacks using GPT, with a past-encoder, present-decoder setup targeting context + claim pairs.  
- **12:20 – Fixer**: Three fixes emerged:  
  1. Handle multiple propositions (e.g., conjunctions) with multi-hop reasoning, filtering unverifiable claims.  
  2. Temporal reasoning via date manipulation and multi-hop logic.  
  3. Tackle ambiguity and lexical variation with entity disambiguation and lexical substitution.  

The fixer leaned on TF-IDF, pointer networks, and joint pointer networks, with post-processing for temporal data. A document-ranking task followed—wish I’d noted the winners!

---

#### 14:00 – Spanish Clinical NLP: Non-English Challenges  
This session focused on unstructured clinical data in Spanish, compared to English tools like cTAKES. A shared task targeted concept recognition (ICD, SNOMED) with annotated corpora. Highlights:  

- Data from hospitals, libraries, and agencies (AEMPS).  
- Spanish-English MT with specialized parallel corpora.  
- Issues: Catalan-Spanish mix-ups, telegraphic sentences, manual labeling woes.  

CUTEXT, TensorFlow, and NLTK powered the effort, with word and character embeddings. Explainable AI, data sharing, and bias in tabular lab data were flagged as next steps.

---

#### 14:40 – AI-Assisted Grading with Rubrics  
AI-assisted grading for essays and short answers was up next. Feature-based systems (n-grams, UMLS similarity) faced off against BERT (WordPiece, PubMed-trained). BERT didn’t crush it—simple linear classifiers held strong, suggesting it needs more data. Evaluation spanned note and offset levels—practical stuff for automating feedback.

---

#### 15:05 – Suicide Notes: NLP for Mental Health  
A sobering session on suicide and depression notes. A dilated LSTM with attention (skip connections for long-term dependencies) tackled texts up to 1,000 words. Visualizing high-attention words was a powerful touch—showing what the model “sees” in these heavy statements.

---

#### 16:00 – Clinical Concepts: Beyond Lexical Matching  
Using the MIMIC-III dataset, this talk contrasted conceptual vs. lexical analysis. Pseudo-sentences (split by line breaks) were tokenized with spaCy. SNOMED CT and LOINC guided terminology, but ECG reports had tons of docs yet few unique concepts. Shared concepts had high confidence, unlike case management and nutrition (low correlation). Embeddings caught template patterns, but metadata—like Social Security numbers—was a nightmare.

---

#### 16:25 – UMLS for Medical Entity Extraction  
Medical entity extraction with UMLS used I2B2 and MedMentions datasets (21 concepts). NCBI-BERT beat BERT BASE, especially in a concatenated general + medical model. Error analysis showed overlapping spans as a pain point—right label, wrong boundaries.

---

#### 16:50 – ICD-9 Ontological Attention  
ICD-9 coding is slow and error-prone, with highly imbalanced data. An ontological attention model (grandparent, parent, specific nodes) with active learning (human-in-the-loop) tackled it. F1 macro-average tracked performance, with dampening for imbalance. Ontology-driven attention feels like a smart evolution.

---

#### 17:15 – Tokenization: Negation and Speculation  
The day wrapped with negation and speculation detection. Regex is easy for basics, but pinpointing *which part* of a sentence matters is linguistic gold. GloVe, ELMo, and fine-tuned BERT (with POS, dependency paths, Bi-LSTM) powered it. Punctuation’s role stood out—small but mighty.

---

### Wrap-Up  
Sunday was a marathon of NLP innovation—cross-document annotation, fact-checking, clinical applications, and beyond. Themes like multimodal/multilingual support, bias, and explainability kept popping up. I’m already excited for Day 2—Spanish pharma embeddings and graph models await! These notes will be my lifeline when I revisit EMNLP-IJCNLP 2019’s brilliance.

---

### Blog Post: Day 2 at EMNLP-IJCNLP 2019 – Monday’s NLP Deep Dive  
*Monday, November 4, 2019 – Second Day of the Conference*  

Day 2 of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) in Hong Kong was a whirlwind of technical sessions. From Spanish pharmaceutical NLP to graph-based embeddings and multi-hop QA, Monday packed a punch. Here’s my recap based on the notes I scribbled down—perfect for revisiting later!

---

#### 9:10 – Spanish Pharma Entity Embeddings  
The morning kicked off with a focus on Spanish pharmaceutical entity embeddings, targeting drugs like antibiotics. A manual annotation guideline guided the process, with a tagger normalizing drug names. BERT and NLTK were in play, but a lingering issue: no one seemed to double-check the Spanish linguistics. A reminder that language-specific nuances matter!

---

#### 9:30 – Pooled Contextual Embeddings  
Next up: pooled contextual embeddings—reproducible, efficient, and flexible. Using Flair and a Bi-LSTM-CRF setup, the approach leveraged Spanish Wikipedia for pooled embeddings. Byte-pair subword embeddings and specialized models like FastText and Wang2Vec added depth. It’s a clever way to balance context and scalability.

---

#### 9:40 – VSP Pharma: NER and Normalization  
The VSP Pharma talk tackled named entity recognition (NER) for drugs and medications, followed by concept normalization using a SNOMED CT search engine. SpaCy handled sentence splitting and word tokenization. A naive first pass picked concepts proportional to instance frequency, boosted by pretrained embeddings as features. Simple yet effective.

---

#### 9:50 – Ixamed: NER with Flair  
Ixamed built on a 50M-word Spanish Wikipedia corpus for NER, again normalizing to SNOMED CT. Levenshtein distance helped measure similarity—a practical touch for handling messy real-world data. Flair powered the system, showing its versatility in domain-specific tasks.

---

#### 10:00 – NLNDE: No Experts Needed?  
The “Neither Language Nor Domain Experts” (NLNDE) session from Bosch was intriguing. A Bi-LSTM-CRF with attention and a noisy channel approach used a CRF in BIO format (9 labels). Embeddings ranged from character and byte-pair to FastText, covering general and bio domains. Unlabeled docs fed the noisy channel—hinting that domain-specific tuning might not always be essential. Bold claim!

---

#### 10:10 – Deep Learning for PharmaCoNER  
The PharmaCoNER talk dove into deep learning: BERT encoded sentences, concatenated embeddings fed concept indexing, and a candidate selection method (CSM) narrowed options. An ablation study highlighted character embeddings and tokenization as critical (40% abbreviations, 20% SNOMED CT coverage). Joint learning tied it together—tokenization’s impact stood out.

---

#### 11:00 – Graph Word Embeddings: WordGraph2Vec  
This session blended linear text embeddings (Word2Vec, GloVe) with graph-based ones, introducing WordGraph2Vec. Built on Node2Vec and DeepWalk, it used random walks and transition probabilities to capture 1st- and 2nd-order proximity. Stopwords and low-frequency words were marked as “unknown.”  

Example: “The car is red” and “The car is hot” got noisy variants, but WordGraph2Vec stayed stable, correcting ~10% of Word2Vec’s errors. Syntax graphs added another layer—promising for analogy and doc classification tasks.

---

#### 11:20 – Multi-Hop QA: Machine Reading Comprehension  
Multi-hop question answering (QA) tackled info spanning two documents via graph-structured representations and doc-to-doc links. Prefiltering grabbed the *k* most relevant sentences (based on *m* similar words to the question). Supporting fact identification leaned on HotpotQA, with BERT experiments outperforming an RNN + attention baseline. Stanford’s coreference resolution tied it together—multi-hop’s complexity shone through.

---

#### 11:40 – Essentia: Domain-Specific Paraphrasing  
Essentia addressed domain-specific paraphrasing—same meaning, different expressions. Current methods need large annotated datasets and falter outside general domains (e.g., PPDB). This approach worked with few labeled pairs, building sentence graphs via word alignment (similar words, context evidence). A paraphrase generator followed.  

Tested on HotelQA and Snips, it boosted recall but had high false positives. Negation handling needs work—check the RIT GitHub for Essentia details later!

---

#### 11:55 – Layerwise Convolutional Graph Networks  
This talk explored interpretability in graph networks using layerwise relevance backpropagation. Weight contributions and adjacency matrices tracked node/edge relevance, validated on PubMed’s 20K RCT dataset. Occlusion (deleting edges) tested negative evidence—a neat way to trace network dynamics.

---

#### 12:10 – MRQA: Baidu’s D-Net  
The final session covered machine reading QA with Baidu’s D-Net, blending pretraining and fine-tuning (BERT, XLNet, ERNIE). Multi-task learning included masked language modeling (MLM), natural language inference (NLI), and paragraph ranking. Oddly, MTL didn’t boost paragraph ranking or NLI much. PaddleNLP and PaddlePaddle powered it—Baidu’s ecosystem in action.

---

### Reflections  
Monday was a rollercoaster—Spanish pharma NLP dominated the morning, then graph embeddings, multi-hop QA, and paraphrasing took over. Recurring themes: domain specificity, embedding innovation, and the push for efficiency with less data. WordGraph2Vec’s stability and Essentia’s paraphrase graphs were highlights. Day 2 cemented why EMNLP-IJCNLP 2019 was worth the trip—can’t wait to dig into these ideas again!

---
