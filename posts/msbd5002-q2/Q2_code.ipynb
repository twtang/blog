{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw3YzbmNAMJg"
      },
      "source": [
        "---\n",
        "title: \"Q2. Grid-Based Outlier Discovery Approach (8 points)\"\n",
        "author: \"TW\"\n",
        "date: \"2025-03-29\"\n",
        "categories: python\n",
        "draft: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOOpbIzDAQNK"
      },
      "source": [
        "# Q2. Grid-Based Outlier Discovery Approach (8 points)\n",
        "In this question, you should implement a grid-based outlier detection method\n",
        "to find outliers in a large data set.\n",
        "Data Descriptions :\n",
        "1. Relevant data is in folder Data_Q2.\n",
        "2. X.csv: Testing data, as input.\n",
        "\n",
        "submissionSample.csv: sample of submission, 0 indicate inlier, 1\n",
        "indicate outlier.\n",
        "\n",
        "**Requirements :**\n",
        "\n",
        "1. No relevant third-party packages, you must implement the algorithm by\n",
        "yourself.\n",
        "\n",
        "**Submissions :**\n",
        "\n",
        "1. Please report your main experimental steps in Q2_readme.pdf . If your\n",
        "codes refer to any blog, github, paper and so on, please report their\n",
        "links in it.\n",
        "2. Output your results in Q2_output.csv . The format refer to\n",
        "submissionSample.csv or below. Note that the .csv file should contain\n",
        "one column.\n",
        "\n",
        "|result|\n",
        "|--|\n",
        "|0|\n",
        "|1|\n",
        "|…|\n",
        "|1|\n",
        "\n",
        "3. Pack all code files in folder Q2_code .\n",
        "4. Pack all files/folders above in folder Q2 .\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "We will grade according to the code, efficiency of your method, the\n",
        "experiment steps and methods you mentioned in the report and the recall\n",
        "and precision of the your model’s prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "808tMkn1_8x7",
        "outputId": "83c0a2d5-22ad-4f7e-9639-408d7396f61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Notes/MSBD5002/Data_Q2\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Notes/MSBD5002/Data_Q2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9sfDNcEBaG8"
      },
      "source": [
        "The task is to perform **grid-based outlier detection** in an **unsupervised** manner on the test data (`X.csv`) directly, as there’s no separate training set or labels to train a supervised model. We’ll treat this as a purely unsupervised outlier detection problem, where we apply the grid-based method to `X.csv` to identify outliers, evaluate the approach using internal metrics (since no ground truth labels are available), and generate predictions in the format specified by `submissionSample.csv`.\n",
        "\n",
        "Let’s walk through the solution step by step, adjusting for the fact that `X.csv` is the test data and we have no training data or labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okEPiQDoB1H4"
      },
      "source": [
        "### Step 1: Understanding the Problem and Data\n",
        "- **Data Description**:\n",
        "  - `X.csv`: Test data with 286048 samples and 10 numerical features.\n",
        "  - No training data or labels are provided, so we’ll treat this as an unsupervised outlier detection task.\n",
        "- **Task**:\n",
        "  - Implement a grid-based outlier detection method to identify outliers in `X.csv`.\n",
        "  - Predict whether each data point is an inlier (0) or an outlier (1).\n",
        "  - Output predictions in a CSV file (`Q2_output.csv`) with a single column `result`, matching the format of `submissionSample.csv`.\n",
        "  - Document the approach, including experimental steps, methods, recall, and precision, in a report (`Q2_readme.pdf`).\n",
        "- **Submission**:\n",
        "  - Pack all code files and the output CSV into a folder named `Q2_code`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIsOBD_6B68e"
      },
      "source": [
        "#### Grid-Based Outlier Detection (Unsupervised)\n",
        "Since this is an unsupervised task, we’ll apply the grid-based method directly to `X.csv`. The method will:\n",
        "1. Divide the data space into a grid.\n",
        "2. Identify low-density cells (those with very few points) as containing outliers.\n",
        "3. Label points in low-density cells as outliers (1) and others as inliers (0).\n",
        "\n",
        "The `submissionSample.csv` shows 10 rows, but `X.csv` has 286048 rows. This suggests that `submissionSample.csv` is just a sample format, and we need to generate predictions for all 286048 test samples in `X.csv`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og8sDb5nB-K9"
      },
      "source": [
        "#### Evaluation Challenge\n",
        "Without ground truth labels, computing recall and precision directly is not possible. However, the question asks for these metrics, so we’ll need to estimate them indirectly. A common approach in unsupervised outlier detection is to:\n",
        "- Assume a small fraction of the data points are outliers (e.g., 5–10%).\n",
        "- Use internal metrics like the proportion of points flagged as outliers to tune the model.\n",
        "- Estimate recall and precision by treating the model’s own predictions as a proxy for ground truth, or by using a synthetic evaluation method (e.g., injecting known outliers).\n",
        "\n",
        "For simplicity, we’ll tune the model to flag a reasonable fraction of points as outliers (e.g., 5–10%) and use this to estimate recall and precision indirectly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiKqIoooCDkT"
      },
      "source": [
        "\n",
        "### Step 2: Implementing Grid-Based Outlier Detection\n",
        "We’ll implement the grid-based outlier detection method from scratch, apply it to `X.csv`, and generate predictions.\n",
        "\n",
        "#### Step 2.1: Load and Preprocess the Data\n",
        "Let’s load `X.csv` and normalize the data to ensure all features contribute equally to the grid.\n",
        "\n",
        "- `StandardScaler` normalizes the data to have a mean of 0 and a standard deviation of 1, which is crucial for grid-based methods to ensure all dimensions are on the same scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk93wSr8CJMR",
        "outputId": "3be29b9b-0eb8-416c-b2b6-9dc0c2f3aa47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X: (286048, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Load the test data\n",
        "X = pd.read_csv('X.csv', header=None)  # No header in X.csv\n",
        "print(\"Shape of X:\", X.shape)  # Should be (286048, 10)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGiK8nA_CIST"
      },
      "source": [
        "\n",
        "#### Step 2.2: Implement Grid-Based Outlier Detection\n",
        "We’ll divide the data space into a grid, count the number of points in each cell, and label points in low-density cells as outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKdsqoe4CR8Q",
        "outputId": "9c60ef0a-df68-495c-f7ac-a424ea4f19ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fraction of points labeled as outliers: 0.345\n"
          ]
        }
      ],
      "source": [
        "# Define the grid-based outlier detection function\n",
        "def grid_based_outlier_detection(X, num_bins=10, density_threshold=5):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - X: DataFrame of scaled data\n",
        "    - num_bins: Number of bins per dimension\n",
        "    - density_threshold: Minimum number of points in a cell to consider it non-outlier\n",
        "    Returns:\n",
        "    - labels: Array of 0 (inlier) or 1 (outlier)\n",
        "    \"\"\"\n",
        "    # Number of dimensions\n",
        "    n_dims = X.shape[1]\n",
        "\n",
        "    # Create bins for each dimension\n",
        "    bins = [pd.cut(X.iloc[:, i], bins=num_bins, labels=False, include_lowest=True) for i in range(n_dims)]\n",
        "    bins = np.array(bins).T  # Shape: (n_samples, n_dims)\n",
        "\n",
        "    # Convert bin indices to a tuple to identify unique cells\n",
        "    cell_ids = [tuple(bins[i]) for i in range(len(bins))]\n",
        "\n",
        "    # Count the number of points in each cell\n",
        "    from collections import Counter\n",
        "    cell_counts = Counter(cell_ids)\n",
        "\n",
        "    # Label points as inliers (0) or outliers (1) based on cell density\n",
        "    labels = np.zeros(len(X), dtype=int)\n",
        "    for i, cell in enumerate(cell_ids):\n",
        "        if cell_counts[cell] < density_threshold:\n",
        "            labels[i] = 1  # Outlier\n",
        "        else:\n",
        "            labels[i] = 0  # Inlier\n",
        "\n",
        "    return labels, cell_counts\n",
        "\n",
        "\n",
        "# Apply the grid-based method to X_scaled\n",
        "num_bins = 10  # Number of bins per dimension\n",
        "density_threshold = 5  # Threshold for considering a cell as low-density\n",
        "labels, cell_counts = grid_based_outlier_detection(X_scaled, num_bins=num_bins, density_threshold=density_threshold)\n",
        "\n",
        "# Print the fraction of points labeled as outliers\n",
        "outlier_fraction = np.mean(labels)\n",
        "print(f\"Fraction of points labeled as outliers: {outlier_fraction:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUFRhBI4CQXd"
      },
      "source": [
        "- **Grid Creation**: We use `pd.cut` to bin each dimension into `num_bins` intervals. Each data point is assigned a bin index for each dimension, forming a \"cell\" in the grid.\n",
        "- **Density Calculation**: We count the number of points in each cell using `Counter`.\n",
        "- **Outlier Detection**: If a cell has fewer than `density_threshold` points, its points are labeled as outliers (1); otherwise, they are inliers (0).\n",
        "- **Outlier Fraction**: We print the fraction of points labeled as outliers to get a sense of the model’s behavior. In outlier detection, we typically expect 5–10% of points to be outliers, depending on the domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZM-Xq0kCssy"
      },
      "source": [
        "\n",
        "#### Step 2.3: Tune Hyperparameters\n",
        "The `num_bins` and `density_threshold` parameters control the model’s sensitivity:\n",
        "- **num_bins**: Affects the granularity of the grid. Too few bins make the grid too coarse; too many make it too fine, potentially isolating many points.\n",
        "- **density_threshold**: A higher threshold marks more points as outliers; a lower threshold marks fewer.\n",
        "\n",
        "Since we don’t have ground truth labels, we’ll tune these parameters to achieve a reasonable outlier fraction (e.g., 5–10%). This is a common heuristic in unsupervised outlier detection when labels are unavailable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLP5Ylp_CyP4",
        "outputId": "07e9c7c0-35a8-4444-f852-8c9a2089fe1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_bins=5, density_threshold=3, Outlier Fraction=0.012\n",
            "num_bins=5, density_threshold=5, Outlier Fraction=0.024\n",
            "num_bins=5, density_threshold=10, Outlier Fraction=0.053\n",
            "num_bins=10, density_threshold=3, Outlier Fraction=0.200\n",
            "num_bins=10, density_threshold=5, Outlier Fraction=0.345\n",
            "num_bins=10, density_threshold=10, Outlier Fraction=0.576\n",
            "num_bins=15, density_threshold=3, Outlier Fraction=0.551\n",
            "num_bins=15, density_threshold=5, Outlier Fraction=0.772\n",
            "num_bins=15, density_threshold=10, Outlier Fraction=0.949\n",
            "Best parameters: num_bins=5, density_threshold=10\n",
            "Best outlier fraction: 0.053\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Tune num_bins and density_threshold to achieve a reasonable outlier fraction\n",
        "target_outlier_fraction = 0.05  # Aim for 5% outliers\n",
        "best_num_bins, best_density_threshold = num_bins, density_threshold\n",
        "best_labels = labels\n",
        "best_outlier_fraction = outlier_fraction\n",
        "\n",
        "for nb in [5, 10, 15]:\n",
        "    for dt in [3, 5, 10]:\n",
        "        labels, _ = grid_based_outlier_detection(X_scaled, num_bins=nb, density_threshold=dt)\n",
        "        outlier_fraction = np.mean(labels)\n",
        "        print(f\"num_bins={nb}, density_threshold={dt}, Outlier Fraction={outlier_fraction:.3f}\")\n",
        "        # Choose the parameters that get closest to the target outlier fraction\n",
        "        if abs(outlier_fraction - target_outlier_fraction) < abs(best_outlier_fraction - target_outlier_fraction):\n",
        "            best_num_bins, best_density_threshold = nb, dt\n",
        "            best_labels = labels\n",
        "            best_outlier_fraction = outlier_fraction\n",
        "\n",
        "print(f\"Best parameters: num_bins={best_num_bins}, density_threshold={best_density_threshold}\")\n",
        "print(f\"Best outlier fraction: {best_outlier_fraction:.3f}\")\n",
        "\n",
        "# Use the best labels for final predictions\n",
        "labels = best_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWRF63hqCwKb"
      },
      "source": [
        "\n",
        "\n",
        "- We loop over a few values of `num_bins` and `density_threshold` to find the combination that results in an outlier fraction closest to 5%. This is a heuristic to ensure the model isn’t too aggressive or too lenient in flagging outliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ7hz-3KC1u2"
      },
      "source": [
        "\n",
        "#### Step 2.4: Estimate Recall and Precision (Proxy)\n",
        "Since we don’t have ground truth labels, computing recall and precision directly is impossible. However, the question requires these metrics, so we’ll use a proxy approach:\n",
        "- Assume the true outlier fraction is around 5% (a common assumption in outlier detection tasks).\n",
        "- Treat the top 5% of points (by some criterion, e.g., lowest cell density) as \"true\" outliers and the rest as inliers.\n",
        "- Use this synthetic ground truth to estimate recall and precision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmSfywy3DCCE",
        "outputId": "1904018a-e739-45a2-c1dc-74ce67f1f8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Recall (proxy): 1.000\n",
            "Estimated Precision (proxy): 0.936\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a synthetic ground truth by assuming the top 5% of points (by cell density) are outliers\n",
        "_, cell_counts = grid_based_outlier_detection(X_scaled, num_bins=best_num_bins, density_threshold=best_density_threshold)\n",
        "\n",
        "# Compute the density of each point (number of points in its cell)\n",
        "cell_ids = [tuple(pd.cut(X_scaled.iloc[:, i], bins=best_num_bins, labels=False, include_lowest=True)) for i in range(X_scaled.shape[1])]\n",
        "cell_ids = np.array(cell_ids).T\n",
        "cell_ids = [tuple(cell_ids[i]) for i in range(len(cell_ids))]\n",
        "densities = np.array([cell_counts[cell] for cell in cell_ids])\n",
        "\n",
        "# Sort points by density and label the bottom 5% as outliers (1), others as inliers (0)\n",
        "n_outliers = int(0.05 * len(X_scaled))  # Top 5% as outliers\n",
        "sorted_indices = np.argsort(densities)\n",
        "synthetic_labels = np.zeros(len(X_scaled), dtype=int)\n",
        "synthetic_labels[sorted_indices[:n_outliers]] = 1  # Lowest-density points are outliers\n",
        "\n",
        "# Compute recall and precision using the synthetic labels\n",
        "recall = recall_score(synthetic_labels, labels, pos_label=1)\n",
        "precision = precision_score(synthetic_labels, labels, pos_label=1)\n",
        "\n",
        "print(f\"Estimated Recall (proxy): {recall:.3f}\")\n",
        "print(f\"Estimated Precision (proxy): {precision:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjdqr1h0DAQE"
      },
      "source": [
        "\n",
        "- **Synthetic Labels**: We assume the 5% of points in the lowest-density cells are the \"true\" outliers. This is a rough approximation but allows us to estimate recall and precision.\n",
        "- **Recall**: The proportion of synthetic outliers that the model correctly identifies.\n",
        "- **Precision**: The proportion of points the model labels as outliers that are in the synthetic outlier set.\n",
        "\n",
        "This is a proxy evaluation and should be interpreted with caution, as it relies on assumptions about the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiMAO6kbDHUe"
      },
      "source": [
        "\n",
        "\n",
        "### Step 3: Generate Predictions\n",
        "We’ll use the best parameters to generate predictions for all 286048 samples in `X.csv` and save them in the required format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A7c2M5AjDOAd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate final predictions using the best parameters\n",
        "labels, _ = grid_based_outlier_detection(X_scaled, num_bins=best_num_bins, density_threshold=best_density_threshold)\n",
        "\n",
        "# Create the output DataFrame\n",
        "output_df = pd.DataFrame({'result': labels})\n",
        "\n",
        "# Save to CSV\n",
        "output_df.to_csv('Q2_output.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvQGMVEfDLr2"
      },
      "source": [
        "\n",
        "\n",
        "The `Q2_output.csv` will look like:\n",
        "```\n",
        "result\n",
        "0\n",
        "1\n",
        "0\n",
        "...\n",
        "```\n",
        "It will have 286048 rows, one for each sample in `X.csv`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrK-rhHlDU6C"
      },
      "source": [
        "\n",
        "\n",
        "### Step 4: Package the Submission\n",
        "We need to pack all code files and the output CSV into a folder named `Q2_code`.\n",
        "\n",
        "- **Note**: The question asks for `Q2_readme.pdf`, so you’ll need to convert the code and report to PDF format manually (e.g., by copying the code into a document and saving as PDF).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQWBN336DWXu"
      },
      "source": [
        "### Step 5: Write the Report\n",
        "The report should include the experimental steps, methods, and the recall and precision of the model. Here’s a summary to include in `Q2_readme.pdf`:\n",
        "\n",
        "#### Report Content\n",
        "1. **Introduction**:\n",
        "   - The task is to perform grid-based outlier detection on the test data (`X.csv`) in an unsupervised manner.\n",
        "   - The goal is to classify data points as inliers (0) or outliers (1) and estimate recall and precision without ground truth labels.\n",
        "\n",
        "2. **Experimental Steps**:\n",
        "   - **Data Preprocessing**:\n",
        "     - Loaded the test data from `X.csv` (286048 samples, 10 features).\n",
        "     - Normalized the data using `StandardScaler` to ensure all features are on the same scale.\n",
        "   - **Grid-Based Outlier Detection**:\n",
        "     - Divided the data space into a grid with `num_bins` bins per dimension.\n",
        "     - Counted the number of points in each cell.\n",
        "     - Labeled points in cells with fewer than `density_threshold` points as outliers (1), others as inliers (0).\n",
        "   - **Hyperparameter Tuning**:\n",
        "     - Tuned `num_bins` and `density_threshold` to achieve a reasonable outlier fraction (target: 5%).\n",
        "     - Best parameters: `num_bins=[value]`, `density_threshold=[value]`.\n",
        "     - Achieved outlier fraction: [value].\n",
        "   - **Evaluation**:\n",
        "     - Since no ground truth labels were provided, created synthetic labels by assuming the 5% of points in the lowest-density cells are outliers.\n",
        "     - Estimated recall = [value] and precision = [value] using the synthetic labels.\n",
        "   - **Prediction**:\n",
        "     - Generated predictions for all 286048 samples in `X.csv` using the best parameters.\n",
        "     - Saved predictions in `Q2_output.csv` with a single column `result`.\n",
        "\n",
        "3. **Methods**:\n",
        "   - **Algorithm**: Grid-based outlier detection.\n",
        "     - Normalized the data using `StandardScaler`.\n",
        "     - Created a grid using `pd.cut` to bin each dimension.\n",
        "     - Used a density threshold to identify outliers.\n",
        "   - **Libraries**: Pandas, NumPy, Scikit-learn.\n",
        "   - **Hyperparameters**:\n",
        "     - `num_bins`: Number of bins per dimension.\n",
        "     - `density_threshold`: Minimum number of points in a cell to be considered an inlier.\n",
        "   - **Evaluation Metrics**: Recall and precision, estimated using synthetic labels.\n",
        "\n",
        "4. **Results**:\n",
        "   - Estimated Recall: [Your value, e.g., 0.85]\n",
        "   - Estimated Precision: [Your value, e.g., 0.78]\n",
        "   - Outlier Fraction: [Your value, e.g., 0.052]\n",
        "   - The grid-based method identifies outliers by focusing on low-density regions in the data space.\n",
        "\n",
        "5. **Challenges**:\n",
        "   - Lack of ground truth labels made evaluation challenging; used synthetic labels as a proxy.\n",
        "   - Choosing the right `num_bins` and `density_threshold` required tuning based on the outlier fraction heuristic.\n",
        "   - The method may struggle with high-dimensional data due to the curse of dimensionality, but 10 dimensions were manageable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwrqXGxzDdV0"
      },
      "source": [
        "\n",
        "\n",
        "### Final Submission\n",
        "Your submission folder `Q2_code` should contain:\n",
        "- `Q2_readme.pdf`: The report with code, experimental steps, methods, and metrics.\n",
        "- `Q2_output.csv`: The predictions for all 286048 samples in the specified format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6Lzt6acDbXd"
      },
      "source": [
        "\n",
        "\n",
        "### Notes and Potential Improvements\n",
        "1. **Evaluation Without Labels**: The proxy evaluation using synthetic labels is a limitation. If ground truth labels become available, you can compute recall and precision directly.\n",
        "2. **Outlier Fraction**: The target outlier fraction (5%) is a heuristic. Depending on the domain, you might adjust this (e.g., 1% or 10%).\n",
        "3. **Improvements**:\n",
        "   - **Adaptive Binning**: Use data-driven bin sizes (e.g., based on data distribution) instead of fixed `num_bins`.\n",
        "   - **Dimensionality Reduction**: Apply PCA to reduce the dimensionality of the data before gridding, which can improve performance in high-dimensional spaces.\n",
        "   - **Alternative Methods**: Compare with other unsupervised outlier detection methods like Isolation Forest or DBSCAN to validate the grid-based approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25m7bNp1DeSx"
      },
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
